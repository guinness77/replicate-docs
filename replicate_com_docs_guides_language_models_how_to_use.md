Docs

Search documentation`âŒ˜K`

Get started

Collapse sidebar

[Run a model from Node.js](https://replicate.com/docs/get-started/nodejs) [Run a model from Google Colab](https://replicate.com/docs/get-started/google-colab) [Run a model from Python](https://replicate.com/docs/get-started/python) [Fine-tune an image model](https://replicate.com/docs/get-started/fine-tune-with-flux)

Guides

[Build a website with Next.js](https://replicate.com/docs/guides/nextjs) [Build a Discord bot with Python](https://replicate.com/docs/guides/discord-bot) [Build an app with SwiftUI](https://replicate.com/docs/guides/swiftui) [Cache images with Cloudflare](https://replicate.com/docs/guides/cloudflare-image-cache) [Use realtime speech with OpenAI](https://replicate.com/docs/guides/openai-realtime) [Push your own model](https://replicate.com/docs/guides/push-a-model) [Push a Diffusers model](https://replicate.com/docs/guides/push-a-diffusers-model) [Push a Transformers model](https://replicate.com/docs/guides/push-a-transformers-model) [Handle webhooks with Val Town](https://replicate.com/docs/guides/build-a-webhook-notifier-with-val-town) [Deploy a custom model](https://replicate.com/docs/guides/deploy-a-custom-model) [Push a model using GitHub Actions](https://replicate.com/docs/guides/push-a-model-using-github-actions) [Set up a CI/CD pipeline](https://replicate.com/docs/guides/continuous-model-deployment) [Get a GPU on Brev](https://replicate.com/docs/guides/get-a-gpu-on-brev) [Get a GPU on Lambda Labs](https://replicate.com/docs/guides/get-a-gpu-on-lambda-labs) [Working with LoRAs](https://replicate.com/docs/guides/working-with-loras)

ComfyUI

[Craft generative AI workflows with ComfyUI](https://replicate.com/docs/guides/comfyui) [Use ComfyUI manager](https://replicate.com/docs/guides/comfyui/comfyui-manager) [Start by running the ComfyUI examples](https://replicate.com/docs/guides/comfyui/examples) [Popular ComfyUI custom nodes](https://replicate.com/docs/guides/comfyui/custom-nodes) [Run your ComfyUI workflow on Replicate](https://replicate.com/docs/guides/comfyui/run-comfyui-on-replicate) [Run ComfyUI with an API](https://replicate.com/docs/guides/comfyui/run-comfyui-with-an-api)

Hypermedia

[Build AI apps fast with hypermedia](https://replicate.com/docs/guides/hypermedia) [What is hypermedia, and why use it for AI apps?](https://replicate.com/docs/guides/hypermedia/what-is-hypermedia) [Building a face swapping app with Val Town, HTMX, and Replicate](https://replicate.com/docs/guides/hypermedia/build-hypermedia-app)

Instant ID

[Make images of real people instantly with InstantID](https://replicate.com/docs/guides/instant-id) [Run InstantID with an API](https://replicate.com/docs/guides/instant-id/run-instant-id-with-an-api)

Language models

[Prompt and run open-source large language models (LLMs)](https://replicate.com/docs/guides/language-models) [How to use open source language models](https://replicate.com/docs/guides/language-models/how-to-use) [Use cases for open source language models](https://replicate.com/docs/guides/language-models/use-cases) [Popular open source language models and their use cases](https://replicate.com/docs/guides/language-models/popular-models) [How to prompt open source large language models](https://replicate.com/docs/guides/language-models/how-to-prompt) [Advanced prompting for open source large language models](https://replicate.com/docs/guides/language-models/advanced-prompting)

Llava

[Talk to images with Llava 13B](https://replicate.com/docs/guides/llava)

Model best practices

[Best practices for Replicate models](https://replicate.com/docs/guides/model-best-practices)

Photomaker

[Generate photos of real people with Photomaker](https://replicate.com/docs/guides/photomaker)

Stable Diffusion

[Make art with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion) [How to use Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/how-to-use) [Image to image (img2img) with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/image-to-image) [Inpainting with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/inpainting) [Outpainting with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/outpainting) [Fine-tuning Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/fine-tuning) [Using ControlNet with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/controlnet) [Fast Stable Diffusion: Turbo and latent consistency models (LCMs)](https://replicate.com/docs/guides/stable-diffusion/turbo-and-latent-consistency) [A to Z of Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/glossary)

Upscaling images

[Upscale images with AI models](https://replicate.com/docs/guides/upscaling-images) [Upscaling images with Real-ESRGAN](https://replicate.com/docs/guides/upscaling-images/real-esrgan) [Fixing faces with GFPGAN and Codeformer](https://replicate.com/docs/guides/upscaling-images/gfpgan-and-codeformer) [Upscaling images with SwinIR](https://replicate.com/docs/guides/upscaling-images/swinir) [Upscaling images with ControlNet tile](https://replicate.com/docs/guides/upscaling-images/controlnet-tile) [Upscaling images with Ultimate SD Upscale](https://replicate.com/docs/guides/upscaling-images/sd-ultimate-upscale)

Whisper

[Turn speech to text with WhisperX](https://replicate.com/docs/guides/whisper) [Run WhisperX with an API](https://replicate.com/docs/guides/whisper/run-whisperx-with-an-api)

Topics

Models

[About models](https://replicate.com/docs/topics/models) [Run a model](https://replicate.com/docs/topics/models/run-a-model) [Create a model](https://replicate.com/docs/topics/models/create-a-model) [Publish a model](https://replicate.com/docs/topics/models/publish-a-model) [Model versions](https://replicate.com/docs/topics/models/versions) [Model hardware](https://replicate.com/docs/topics/models/hardware) [Private and public models](https://replicate.com/docs/topics/models/private-models) [Training destinations](https://replicate.com/docs/topics/models/models-as-training-destinations) [Delete a model](https://replicate.com/docs/topics/models/delete-a-model)

Predictions

[About predictions](https://replicate.com/docs/topics/predictions) [Create a prediction](https://replicate.com/docs/topics/predictions/create-a-prediction) [Input files](https://replicate.com/docs/topics/predictions/input-files) [Output files](https://replicate.com/docs/topics/predictions/output-files) [Prediction lifecycle](https://replicate.com/docs/topics/predictions/lifecycle) [Share a prediction](https://replicate.com/docs/topics/predictions/share-a-prediction) [Rate limits](https://replicate.com/docs/topics/predictions/rate-limits) [Safety checking](https://replicate.com/docs/topics/predictions/safety-checking) [Data retention](https://replicate.com/docs/topics/predictions/data-retention) [Streaming output](https://replicate.com/docs/topics/predictions/streaming)

Deployments

[About deployments](https://replicate.com/docs/topics/deployments) [Create a deployment](https://replicate.com/docs/topics/deployments/create-a-deployment) [View deployments](https://replicate.com/docs/topics/deployments/view-deployments) [Delete a deployment](https://replicate.com/docs/topics/deployments/delete-a-deployment)

Webhooks

[About webhooks](https://replicate.com/docs/topics/webhooks) [Set up webhooks](https://replicate.com/docs/topics/webhooks/setup-webhook) [Receive webhooks](https://replicate.com/docs/topics/webhooks/receive-webhook) [Verify webhooks](https://replicate.com/docs/topics/webhooks/verify-webhook) [Test your webhook code](https://replicate.com/docs/topics/webhooks/testing-webhook-code)

Organizations

[About organizations](https://replicate.com/docs/topics/organizations)

Billing

[About billing](https://replicate.com/docs/topics/billing)

Site policy

[About subprocessors](https://replicate.com/docs/topics/site-policy/subprocessors)

Reference

[How does Replicate work?](https://replicate.com/docs/reference/how-does-replicate-work) [Client libraries](https://replicate.com/docs/reference/client-libraries) [HTTP API](https://replicate.com/docs/reference/http) [OpenAPI schema](https://replicate.com/docs/reference/openapi) [Open source](https://replicate.com/docs/reference/open-source)

Let's cover the basics of how to use open source language models by exploring the important language model parameters, and how they work.

To work through these examples, you can use a [language model on Replicate, like meta/llama-3.1-405b-instruct](https://replicate.com/meta/meta-llama-3.1-405b-instruct).

- [prompt](https://replicate.com/docs/guides/language-models/how-to-use#prompt)
- [tokens](https://replicate.com/docs/guides/language-models/how-to-use#tokens)
- [context window](https://replicate.com/docs/guides/language-models/how-to-use#context-window)
- [system prompt](https://replicate.com/docs/guides/language-models/how-to-use#system-prompt)
- [temperature](https://replicate.com/docs/guides/language-models/how-to-use#temperature)
- [top\_p](https://replicate.com/docs/guides/language-models/how-to-use#top_p)
- [top\_k](https://replicate.com/docs/guides/language-models/how-to-use#top_k)
- [minimum and maximum tokens](https://replicate.com/docs/guides/language-models/how-to-use#minimum-and-maximum-new-tokens)
- [stop sequences](https://replicate.com/docs/guides/language-models/how-to-use#stop-sequences)
- [frequency penalty](https://replicate.com/docs/guides/language-models/how-to-use#frequency-penalty)

## [Anchor for prompt](https://replicate.com/docs/guides/language-models/how-to-use\#prompt) Prompt

The most important parameter for language models is the `prompt`. The `prompt` is the primary input instruction for the language model. When you feed the prompt to the language model, the language model responds with what it thinks is the most-likely next word.

Try out some of your own prompts below.

> Tell me a story about cats!

## [Anchor for tokens](https://replicate.com/docs/guides/language-models/how-to-use\#tokens) Tokens

Tokens are the foundational unit of language model inputs and outputs. Language models don't see words or characters like you and I -- they see tokens.

For example, the sentence of "Transformers language models are neat!" actually is broken down into 9 tokens: `<s>` ` Trans` `form` `ers` ` language` ` models` ` are` ` neat` and `!`.

Each language model has a distinct vocabulary of tokens, so the exact number of tokens will vary from model to model. For Meta's Llama2 models, the vocabulary consists of 32,000 possible tokens. A token vocabulary is a map of text string to integer id value.

As a rule of thumb, tokens are typically around 4 characters long. But this isnâ€™t always the case, sometimes a single character can count as an entire token.

Below you can visualize how the Llama2 tokenizer chunks words into tokens. You can also play with this yourself on the [llama-tokenizer-js playground](https://belladoreai.github.io/llama-tokenizer-js/example-demo/build/). You'll also notice that this tool adds a `<s>` tokento the beginning of the list of tokens. This this is a special "beginning of sentence token" or `bos_token`, and it is how Llama2 understands that this is the beginning of a prompt. You don't need to include this in your prompt, it will automatically be included by the model tokenizer, but you will occasionally see it mentioned when reading about Llama2

![llama-tokenizer-js playground](https://replicate.com/frontend-assets/llama2-tokenizer-example-CBSXZCfN.png)

## [Anchor for context-window](https://replicate.com/docs/guides/language-models/how-to-use\#context-window) Context window

Language models typically have a limited set of context, meaning they can only handle so many tokens before they start forgetting text they have previously seen or generated. This limit of tokens is called the context window.

Different language models have different sized context windows. For example the Llama2 model has a context window of `4,096` tokens or about 12 pages of text. The leading-edge proprietary models like GPT-4-Turbo have a context window of `128,000` tokens, or about 300 pages of text.

It's important to understand the context window of the model you're using, and how it impacts your use case. For example if you're using Llama2, you won't be able to ingest an entire book and answer questions; you'll need to use additional strategies like [retrieval augmented generation](https://replicate.com/blog/how-to-use-rag-with-chromadb-and-mistral-7b-instruct). While GPT-4-Turbo can read an entire book without skipping a beat.

## [Anchor for system-prompt](https://replicate.com/docs/guides/language-models/how-to-use\#system-prompt) System prompt

The system prompt is a secondary prompt that you can use with [instruction-tuned language models](https://replicate.com/guides/language-models/use-cases#2-instruct-models) to to define the desired "behavior" or "personality" of the language model response.

For example, if you want the language model to only respond with haikus, you can set the system prompt to `Only respond with haikus.`

> It's a beautiful day in the neighborhood, would you be my neighbor?
> system\_prompt: Only respond with haikus.

System prompts are great for defining the "character" of your language model response. For example, this is what happens when we set our system prompt to be `Yarr matey! respond as if ye are a pirate.`

> Describe the free energy principle
> system\_prompt: Yarr matey! respond as if ye are a pirate.

## [Anchor for temperature](https://replicate.com/docs/guides/language-models/how-to-use\#temperature) Temperature

The `temperature` parameter is used to set the randomness of a language model response, by scaling its probability distribution over tokens.

When the `temperature` is set to `0.01`, the language model will always respond with the most likely token.

When the `temperature` is set to `1.0`, the language model's probability distribution is used as-is, without any scaling. While the most likely token is still the most probable choice, there's room for diversity, as the model doesn't always select it.

As the `temperature` increases above `1.0`, the probability distribution is scaled to increase entropy, meaning that lower-probability tokens gain a higher chance of being selected. This leads to more surprising and varied outputs, as the model starts to explore less likely options. High temperatures, such as 5.0, can result in highly random and creative responses, but they may also reduce coherence and relevance.

In summary, a lower `temperature` ( `<1.0`) steers the model towards more deterministic and predictable behavior. A `temperature` of `1.0` offers a balance, using the model's learned probability distribution to guide token selection. Higher temperatures ( `>1.0`) increase randomness and creativity in the outputs.

Experimenting with different `temperature` settings can help you find the optimal balance for your specific use case, depending on whether you prioritize predictability or creativity.

> Write me lyrics for a song about the free energy principle

Copy

```
temperature
- defaultValue: 0.7
- min: 0.1
- max: 5
- step: 0.1
```

## [Anchor for top_p](https://replicate.com/docs/guides/language-models/how-to-use\#top_p) top\_p

The `top_p` parameter is used to control the diversity of a language model's responses by adjusting its word selection process.

When `top_p` is set to a lower value, like 0.1, the language model restricts its choices to a very small set of the most likely tokens. This leads to more predictable and conservative outputs, as it only considers the top 10% of the most probable words in each step.

As `top_p` increases, the model includes a broader range of words in its selection pool, allowing for more varied and creative responses. A top\_p value of `0.9` or higher enables the model to consider a wider array of possibilities, picking from the top 90% of probable words. This setting is useful when you want the language model to generate more diverse and less constrained text.

So, as a general guideline: if you're aiming for more consistent and focused outputs, use a lower `top_p` value. If you're seeking creativity and a wider range of responses, opt for a higher `top_p`.

`top_p` is also influenced by both the `temperature` and `top_k` parameters. Try playing with all of the knobs in the example below.

> Explain Docker but pretend to be a RuneScape wizard

Copy

```
top_p
- defaultValue: 0.95
- min: 0.0
- max: 1.0
- step: 0.01
```

Copy

```
top_k
- defaultValue: -1
- min: -1
- max: 50
- step: 1.0
```

Copy

```
temperature
- defaultValue: 0.7
- min: 0.1
- max: 5
- step: 0.1
```

## [Anchor for top_k](https://replicate.com/docs/guides/language-models/how-to-use\#top_k) top\_k

The `top_k` parameter is a method used to refine the selection process of a language model when it generates text. It limits the number of words or tokens the model considers at each step of the generation process.

When `top_k` is set to a specific value, say 10, the model only considers the top 10 most likely next words or tokens at each step. It essentially ignores all other words in its vocabulary, regardless of their probability. This restriction helps to focus the model's choices and can lead to more predictable and relevant text.

If `top_k` is set to a very high number, the model's behavior begins to resemble more unrestricted, probabilistic generation, as it's allowed to consider a wide range of possible words. Conversely, a very low `top_k` value (like 1 or 2) makes the model's output highly deterministic and less varied.

Therefore, `top_k` is a key parameter for balancing creativity and coherence in text generation. A lower `top_k` leads to safer, more predictable text, while a higher `top_k` allows for more diverse and potentially creative outputs. The ideal `top_k` value often depends on the specific task and desired output characteristics.

> Explain Docker but pretend to be a RuneScape wizard

Copy

```
top_p
- defaultValue: 0.95
- min: 0.0
- max: 1.0
- step: 0.01
```

Copy

```
top_k
- defaultValue: -1
- min: -1
- max: 50
- step: 1.0
```

Copy

```
temperature
- defaultValue: 0.7
- min: 0.1
- max: 5
- step: 0.1
```

## [Anchor for minimum-and-maximum-new-tokens](https://replicate.com/docs/guides/language-models/how-to-use\#minimum-and-maximum-new-tokens) Minimum and maximum new tokens

The `min_new_tokens` and `max_new_tokens` parameters are used to control the length of the generated output.

`min_new_tokens` sets the minimum number of new tokens that the model should generate. This is useful when you need to ensure that the output has a certain amount of substance or detail.

`max_new_tokens` defines the maximum number of tokens the model is allowed to generate. This parameter ensures that the output doesn't exceed a certain length. It's helpful in keeping generated content concise.

> Explain Docker but pretend to be a RuneScape wizard

Copy

```
max_new_tokens
- defaultValue: 128
- min: 1
- max: 1024
- step: 1.0
```

Copy

```
min_new_tokens
- defaultValue: -1
- min: -1
- max: 1024
- step: 1.0
```

## [Anchor for stop-sequences](https://replicate.com/docs/guides/language-models/how-to-use\#stop-sequences) Stop sequences

These are sequences of tokens that, when generated, will stop the model from generating any more text.

This is useful for controlling the length and relevance of the output. For example, if you set the stop sequence to a period ".", the model will stop generating text once it completes a sentence.

Without a stop sequence the model can continue generating text up to its maximum token limit, or until it reaches a conclusion. This can lead to long or off-topic responses.

## [Anchor for frequency-penalty-repetition_penalty](https://replicate.com/docs/guides/language-models/how-to-use\#frequency-penalty-repetition_penalty) Frequency penalty (repetition\_penalty)

This parameter controls the amount of repetition in the generated text. It changes the likelihood of repeatedly using the same words or phrases.

With a `repetition_penalty` of 0, there is no penalty, allowing the model to use words as frequently as it needs. This can sometimes lead to repetitive text, especially in longer outputs.

If you're noticing too much repetition in the model's output, increasing the `repetition_penalty` can help. A higher setting, generally ranging from 0.1 to 1, imposes a stronger penalty on the recurrence of words. This motivates the model to employ a broader range of vocabulary and to write sentences with greater variation.

Do not set the penalty too high. This can lead the model to avoid relevant terms that are necessary for coherent text.

[Next:Use cases for open source language models](https://replicate.com/docs/guides/language-models/use-cases)

# Docs

Get started

Collapse sidebar

[Run a model from Node.js](https://replicate.com/docs/get-started/nodejs) [Run a model from Google Colab](https://replicate.com/docs/get-started/google-colab) [Run a model from Python](https://replicate.com/docs/get-started/python) [Fine-tune an image model](https://replicate.com/docs/get-started/fine-tune-with-flux)

Guides

[Build a website with Next.js](https://replicate.com/docs/guides/nextjs) [Build a Discord bot with Python](https://replicate.com/docs/guides/discord-bot) [Build an app with SwiftUI](https://replicate.com/docs/guides/swiftui) [Cache images with Cloudflare](https://replicate.com/docs/guides/cloudflare-image-cache) [Use realtime speech with OpenAI](https://replicate.com/docs/guides/openai-realtime) [Push your own model](https://replicate.com/docs/guides/push-a-model) [Push a Diffusers model](https://replicate.com/docs/guides/push-a-diffusers-model) [Push a Transformers model](https://replicate.com/docs/guides/push-a-transformers-model) [Handle webhooks with Val Town](https://replicate.com/docs/guides/build-a-webhook-notifier-with-val-town) [Deploy a custom model](https://replicate.com/docs/guides/deploy-a-custom-model) [Push a model using GitHub Actions](https://replicate.com/docs/guides/push-a-model-using-github-actions) [Set up a CI/CD pipeline](https://replicate.com/docs/guides/continuous-model-deployment) [Get a GPU on Brev](https://replicate.com/docs/guides/get-a-gpu-on-brev) [Get a GPU on Lambda Labs](https://replicate.com/docs/guides/get-a-gpu-on-lambda-labs) [Working with LoRAs](https://replicate.com/docs/guides/working-with-loras)

ComfyUI

[Craft generative AI workflows with ComfyUI](https://replicate.com/docs/guides/comfyui) [Use ComfyUI manager](https://replicate.com/docs/guides/comfyui/comfyui-manager) [Start by running the ComfyUI examples](https://replicate.com/docs/guides/comfyui/examples) [Popular ComfyUI custom nodes](https://replicate.com/docs/guides/comfyui/custom-nodes) [Run your ComfyUI workflow on Replicate](https://replicate.com/docs/guides/comfyui/run-comfyui-on-replicate) [Run ComfyUI with an API](https://replicate.com/docs/guides/comfyui/run-comfyui-with-an-api)

Hypermedia

[Build AI apps fast with hypermedia](https://replicate.com/docs/guides/hypermedia) [What is hypermedia, and why use it for AI apps?](https://replicate.com/docs/guides/hypermedia/what-is-hypermedia) [Building a face swapping app with Val Town, HTMX, and Replicate](https://replicate.com/docs/guides/hypermedia/build-hypermedia-app)

Instant ID

[Make images of real people instantly with InstantID](https://replicate.com/docs/guides/instant-id) [Run InstantID with an API](https://replicate.com/docs/guides/instant-id/run-instant-id-with-an-api)

Language models

[Prompt and run open-source large language models (LLMs)](https://replicate.com/docs/guides/language-models) [How to use open source language models](https://replicate.com/docs/guides/language-models/how-to-use) [Use cases for open source language models](https://replicate.com/docs/guides/language-models/use-cases) [Popular open source language models and their use cases](https://replicate.com/docs/guides/language-models/popular-models) [How to prompt open source large language models](https://replicate.com/docs/guides/language-models/how-to-prompt) [Advanced prompting for open source large language models](https://replicate.com/docs/guides/language-models/advanced-prompting)

Llava

[Talk to images with Llava 13B](https://replicate.com/docs/guides/llava)

Model best practices

[Best practices for Replicate models](https://replicate.com/docs/guides/model-best-practices)

Photomaker

[Generate photos of real people with Photomaker](https://replicate.com/docs/guides/photomaker)

Stable Diffusion

[Make art with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion) [How to use Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/how-to-use) [Image to image (img2img) with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/image-to-image) [Inpainting with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/inpainting) [Outpainting with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/outpainting) [Fine-tuning Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/fine-tuning) [Using ControlNet with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/controlnet) [Fast Stable Diffusion: Turbo and latent consistency models (LCMs)](https://replicate.com/docs/guides/stable-diffusion/turbo-and-latent-consistency) [A to Z of Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/glossary)

Upscaling images

[Upscale images with AI models](https://replicate.com/docs/guides/upscaling-images) [Upscaling images with Real-ESRGAN](https://replicate.com/docs/guides/upscaling-images/real-esrgan) [Fixing faces with GFPGAN and Codeformer](https://replicate.com/docs/guides/upscaling-images/gfpgan-and-codeformer) [Upscaling images with SwinIR](https://replicate.com/docs/guides/upscaling-images/swinir) [Upscaling images with ControlNet tile](https://replicate.com/docs/guides/upscaling-images/controlnet-tile) [Upscaling images with Ultimate SD Upscale](https://replicate.com/docs/guides/upscaling-images/sd-ultimate-upscale)

Whisper

[Turn speech to text with WhisperX](https://replicate.com/docs/guides/whisper) [Run WhisperX with an API](https://replicate.com/docs/guides/whisper/run-whisperx-with-an-api)

Topics

Models

[About models](https://replicate.com/docs/topics/models) [Run a model](https://replicate.com/docs/topics/models/run-a-model) [Create a model](https://replicate.com/docs/topics/models/create-a-model) [Publish a model](https://replicate.com/docs/topics/models/publish-a-model) [Model versions](https://replicate.com/docs/topics/models/versions) [Model hardware](https://replicate.com/docs/topics/models/hardware) [Private and public models](https://replicate.com/docs/topics/models/private-models) [Training destinations](https://replicate.com/docs/topics/models/models-as-training-destinations) [Delete a model](https://replicate.com/docs/topics/models/delete-a-model)

Predictions

[About predictions](https://replicate.com/docs/topics/predictions) [Create a prediction](https://replicate.com/docs/topics/predictions/create-a-prediction) [Input files](https://replicate.com/docs/topics/predictions/input-files) [Output files](https://replicate.com/docs/topics/predictions/output-files) [Prediction lifecycle](https://replicate.com/docs/topics/predictions/lifecycle) [Share a prediction](https://replicate.com/docs/topics/predictions/share-a-prediction) [Rate limits](https://replicate.com/docs/topics/predictions/rate-limits) [Safety checking](https://replicate.com/docs/topics/predictions/safety-checking) [Data retention](https://replicate.com/docs/topics/predictions/data-retention) [Streaming output](https://replicate.com/docs/topics/predictions/streaming)

Deployments

[About deployments](https://replicate.com/docs/topics/deployments) [Create a deployment](https://replicate.com/docs/topics/deployments/create-a-deployment) [View deployments](https://replicate.com/docs/topics/deployments/view-deployments) [Delete a deployment](https://replicate.com/docs/topics/deployments/delete-a-deployment)

Webhooks

[About webhooks](https://replicate.com/docs/topics/webhooks) [Set up webhooks](https://replicate.com/docs/topics/webhooks/setup-webhook) [Receive webhooks](https://replicate.com/docs/topics/webhooks/receive-webhook) [Verify webhooks](https://replicate.com/docs/topics/webhooks/verify-webhook) [Test your webhook code](https://replicate.com/docs/topics/webhooks/testing-webhook-code)

Organizations

[About organizations](https://replicate.com/docs/topics/organizations)

Billing

[About billing](https://replicate.com/docs/topics/billing)

Site policy

[About subprocessors](https://replicate.com/docs/topics/site-policy/subprocessors)

Reference

[How does Replicate work?](https://replicate.com/docs/reference/how-does-replicate-work) [Client libraries](https://replicate.com/docs/reference/client-libraries) [HTTP API](https://replicate.com/docs/reference/http) [OpenAPI schema](https://replicate.com/docs/reference/openapi) [Open source](https://replicate.com/docs/reference/open-source)

Close

Popular searches on Replicate

- [Run a model from Node.js](https://replicate.com/docs/get-started/nodejs)
- [Deploy a custom model](https://replicate.com/docs/guides/deploy-a-custom-model)
- [How does Replicate work?](https://replicate.com/docs/reference/how-does-replicate-work)

Showing 0 results