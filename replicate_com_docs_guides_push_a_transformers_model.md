Docs

Search documentation`âŒ˜K`

Get started

Collapse sidebar

[Run a model from Node.js](https://replicate.com/docs/get-started/nodejs) [Run a model from Google Colab](https://replicate.com/docs/get-started/google-colab) [Run a model from Python](https://replicate.com/docs/get-started/python) [Fine-tune an image model](https://replicate.com/docs/get-started/fine-tune-with-flux)

Guides

[Build a website with Next.js](https://replicate.com/docs/guides/nextjs) [Build a Discord bot with Python](https://replicate.com/docs/guides/discord-bot) [Build an app with SwiftUI](https://replicate.com/docs/guides/swiftui) [Cache images with Cloudflare](https://replicate.com/docs/guides/cloudflare-image-cache) [Use realtime speech with OpenAI](https://replicate.com/docs/guides/openai-realtime) [Push your own model](https://replicate.com/docs/guides/push-a-model) [Push a Diffusers model](https://replicate.com/docs/guides/push-a-diffusers-model) [Push a Transformers model](https://replicate.com/docs/guides/push-a-transformers-model) [Handle webhooks with Val Town](https://replicate.com/docs/guides/build-a-webhook-notifier-with-val-town) [Deploy a custom model](https://replicate.com/docs/guides/deploy-a-custom-model) [Push a model using GitHub Actions](https://replicate.com/docs/guides/push-a-model-using-github-actions) [Set up a CI/CD pipeline](https://replicate.com/docs/guides/continuous-model-deployment) [Get a GPU on Brev](https://replicate.com/docs/guides/get-a-gpu-on-brev) [Get a GPU on Lambda Labs](https://replicate.com/docs/guides/get-a-gpu-on-lambda-labs) [Working with LoRAs](https://replicate.com/docs/guides/working-with-loras)

ComfyUI

[Craft generative AI workflows with ComfyUI](https://replicate.com/docs/guides/comfyui) [Use ComfyUI manager](https://replicate.com/docs/guides/comfyui/comfyui-manager) [Start by running the ComfyUI examples](https://replicate.com/docs/guides/comfyui/examples) [Popular ComfyUI custom nodes](https://replicate.com/docs/guides/comfyui/custom-nodes) [Run your ComfyUI workflow on Replicate](https://replicate.com/docs/guides/comfyui/run-comfyui-on-replicate) [Run ComfyUI with an API](https://replicate.com/docs/guides/comfyui/run-comfyui-with-an-api)

Hypermedia

[Build AI apps fast with hypermedia](https://replicate.com/docs/guides/hypermedia) [What is hypermedia, and why use it for AI apps?](https://replicate.com/docs/guides/hypermedia/what-is-hypermedia) [Building a face swapping app with Val Town, HTMX, and Replicate](https://replicate.com/docs/guides/hypermedia/build-hypermedia-app)

Instant ID

[Make images of real people instantly with InstantID](https://replicate.com/docs/guides/instant-id) [Run InstantID with an API](https://replicate.com/docs/guides/instant-id/run-instant-id-with-an-api)

Language models

[Prompt and run open-source large language models (LLMs)](https://replicate.com/docs/guides/language-models) [How to use open source language models](https://replicate.com/docs/guides/language-models/how-to-use) [Use cases for open source language models](https://replicate.com/docs/guides/language-models/use-cases) [Popular open source language models and their use cases](https://replicate.com/docs/guides/language-models/popular-models) [How to prompt open source large language models](https://replicate.com/docs/guides/language-models/how-to-prompt) [Advanced prompting for open source large language models](https://replicate.com/docs/guides/language-models/advanced-prompting)

Llava

[Talk to images with Llava 13B](https://replicate.com/docs/guides/llava)

Model best practices

[Best practices for Replicate models](https://replicate.com/docs/guides/model-best-practices)

Photomaker

[Generate photos of real people with Photomaker](https://replicate.com/docs/guides/photomaker)

Stable Diffusion

[Make art with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion) [How to use Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/how-to-use) [Image to image (img2img) with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/image-to-image) [Inpainting with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/inpainting) [Outpainting with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/outpainting) [Fine-tuning Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/fine-tuning) [Using ControlNet with Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/controlnet) [Fast Stable Diffusion: Turbo and latent consistency models (LCMs)](https://replicate.com/docs/guides/stable-diffusion/turbo-and-latent-consistency) [A to Z of Stable Diffusion](https://replicate.com/docs/guides/stable-diffusion/glossary)

Upscaling images

[Upscale images with AI models](https://replicate.com/docs/guides/upscaling-images) [Upscaling images with Real-ESRGAN](https://replicate.com/docs/guides/upscaling-images/real-esrgan) [Fixing faces with GFPGAN and Codeformer](https://replicate.com/docs/guides/upscaling-images/gfpgan-and-codeformer) [Upscaling images with SwinIR](https://replicate.com/docs/guides/upscaling-images/swinir) [Upscaling images with ControlNet tile](https://replicate.com/docs/guides/upscaling-images/controlnet-tile) [Upscaling images with Ultimate SD Upscale](https://replicate.com/docs/guides/upscaling-images/sd-ultimate-upscale)

Whisper

[Turn speech to text with WhisperX](https://replicate.com/docs/guides/whisper) [Run WhisperX with an API](https://replicate.com/docs/guides/whisper/run-whisperx-with-an-api)

Topics

Models

[About models](https://replicate.com/docs/topics/models) [Run a model](https://replicate.com/docs/topics/models/run-a-model) [Create a model](https://replicate.com/docs/topics/models/create-a-model) [Publish a model](https://replicate.com/docs/topics/models/publish-a-model) [Model versions](https://replicate.com/docs/topics/models/versions) [Model hardware](https://replicate.com/docs/topics/models/hardware) [Private and public models](https://replicate.com/docs/topics/models/private-models) [Training destinations](https://replicate.com/docs/topics/models/models-as-training-destinations) [Delete a model](https://replicate.com/docs/topics/models/delete-a-model)

Predictions

[About predictions](https://replicate.com/docs/topics/predictions) [Create a prediction](https://replicate.com/docs/topics/predictions/create-a-prediction) [Input files](https://replicate.com/docs/topics/predictions/input-files) [Output files](https://replicate.com/docs/topics/predictions/output-files) [Prediction lifecycle](https://replicate.com/docs/topics/predictions/lifecycle) [Share a prediction](https://replicate.com/docs/topics/predictions/share-a-prediction) [Rate limits](https://replicate.com/docs/topics/predictions/rate-limits) [Safety checking](https://replicate.com/docs/topics/predictions/safety-checking) [Data retention](https://replicate.com/docs/topics/predictions/data-retention) [Streaming output](https://replicate.com/docs/topics/predictions/streaming)

Deployments

[About deployments](https://replicate.com/docs/topics/deployments) [Create a deployment](https://replicate.com/docs/topics/deployments/create-a-deployment) [View deployments](https://replicate.com/docs/topics/deployments/view-deployments) [Delete a deployment](https://replicate.com/docs/topics/deployments/delete-a-deployment)

Webhooks

[About webhooks](https://replicate.com/docs/topics/webhooks) [Set up webhooks](https://replicate.com/docs/topics/webhooks/setup-webhook) [Receive webhooks](https://replicate.com/docs/topics/webhooks/receive-webhook) [Verify webhooks](https://replicate.com/docs/topics/webhooks/verify-webhook) [Test your webhook code](https://replicate.com/docs/topics/webhooks/testing-webhook-code)

Organizations

[About organizations](https://replicate.com/docs/topics/organizations)

Billing

[About billing](https://replicate.com/docs/topics/billing)

Site policy

[About subprocessors](https://replicate.com/docs/topics/site-policy/subprocessors)

Reference

[How does Replicate work?](https://replicate.com/docs/reference/how-does-replicate-work) [Client libraries](https://replicate.com/docs/reference/client-libraries) [HTTP API](https://replicate.com/docs/reference/http) [OpenAPI schema](https://replicate.com/docs/reference/openapi) [Open source](https://replicate.com/docs/reference/open-source)

[Transformers](https://huggingface.co/docs/transformers/index) is an open-source Python library that provides a consistent interface for using language models. The library contains multiple open-source generative language models like FLAN, GPT-J, GPT Neo, LLaMA, BLOOM, and others, which have been pre-trained on large text corpora and can be fine-tuned for specific tasks with relatively small amounts of training data.

Transformers also contains models like Longformer, BERT, and RoBERTa, which are generally used for more traditional natural language processing tasks like classification, named entity recognition, and so on. The process we're walking through here will work for both kinds of models; in fact, it should work for every model on Transformers.

In this guide we'll walk you through the process of taking an existing Transformers model and pushing it to Replicate as your own public or private model with a stable API.

## [Anchor for prerequisites](https://replicate.com/docs/guides/push-a-transformers-model\#prerequisites) Prerequisites

To follow this guide, you'll need:

- **An account on Replicate.**
- **Docker.** You'll be using the Cog command-line tool to build and push your model. Cog uses Docker to create a container for your model. You'll need to [install and start Docker](https://docs.docker.com/get-docker/) before you can run Cog. You can confirm Docker is running by typing `docker info` in your terminal.

## [Anchor for step-1-create-a-model](https://replicate.com/docs/guides/push-a-transformers-model\#step-1-create-a-model) Step 1: Create a model

First, create a model on Replicate at [replicate.com/create](https://replicate.com/create?purpose=fine-tune-language). If you haven't used Replicate before, you'll need to sign in with your GitHub account. You can configure the model to be private so that only you can use it, or public so anyone can use it.

[![](https://user-images.githubusercontent.com/2289/222594676-ecb72c3e-1fb4-4ed5-b368-1a82afc46f3b.png)](https://replicate.com/create?purpose=fine-tune-language)

## [Anchor for step-2-install-cog](https://replicate.com/docs/guides/push-a-transformers-model\#step-2-install-cog) Step 2: Install Cog

Cog is an open source tool that makes it easy to put a machine learning model in a Docker container. Run the following commands to install it and set the correct permissions:

Copy

```
curl https://replicate.github.io/codespaces/scripts/install-cog.sh | bash
```

Confirm that Cog is installed by running `cog --version`:

Copy

```
cog --version
# cog version 0.9.25 (built 2024-10-07T15:11:47Z)
```

## [Anchor for step-3-initialize-your-project](https://replicate.com/docs/guides/push-a-transformers-model\#step-3-initialize-your-project) Step 3: Initialize your project

Create a new directory and initialize a new Cog project:

Copy

```
mkdir my-cool-model
cd my-cool-model
cog init
```

This will create two files, `cog.yaml` and `predict.py`, which you'll use to configure your dependencies and define the inputs and outputs of your model.

## [Anchor for step-4-configure-dependencies](https://replicate.com/docs/guides/push-a-transformers-model\#step-4-configure-dependencies) Step 4: Configure dependencies

[The `cog.yaml` file](https://github.com/replicate/cog/blob/main/docs/yaml.md) defines the CUDA and Python versions, and dependencies for the model. This file tells Cog how to package the model.

Replace the contents of the `cog.yaml` file with the following:

Copy

```
build:
  gpu: true
  python_version: "3.10"
  python_packages:
    - "torch==1.12.1"
    - "transformers==4.30.0"
    - "sentencepiece==0.1.97"
    - "accelerate==0.16.0"

predict: "predict.py:Predictor"
```

## [Anchor for step-5-customize-your-predictor](https://replicate.com/docs/guides/push-a-transformers-model\#step-5-customize-your-predictor) Step 5: Customize your predictor

[The `predict.py` file](https://github.com/replicate/cog/blob/main/docs/python.md) defines the inputs and outputs of the model, and the code to run the model. The language model itself is imported through the Python [`transformers`](https://pypi.org/project/transformers/) library.

Replace the contents of the `predict.py` file with the following:

Copy

```
from typing import List, Optional
from cog import BasePredictor, Input
from transformers import T5ForConditionalGeneration, AutoTokenizer
import torch

CACHE_DIR = 'weights'

# Shorthand identifier for a transformers model.
# See https://huggingface.co/models?library=transformers for a list of models.
MODEL_NAME = 'google/flan-t5-xl'

class Predictor(BasePredictor):
    def setup(self):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR, local_files_only=True)
        self.model.to(self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR, local_files_only=True)

    def predict(
        self,
        prompt: str = Input(description=f"Text prompt to send to the model."),
        n: int = Input(description="Number of output sequences to generate", default=1, ge=1, le=5),
        max_length: int = Input(
            description="Maximum number of tokens to generate. A word is generally 2-3 tokens",
            ge=1,
            default=50
        ),
        temperature: float = Input(
            description="Adjusts randomness of outputs, greater than 1 is random and 0 is deterministic, 0.75 is a good starting value.",
            ge=0.01,
            le=5,
            default=0.75,
        ),
        top_p: float = Input(
            description="When decoding text, samples from the top p percentage of most likely tokens; lower to ignore less likely tokens",
            ge=0.01,
            le=1.0,
            default=1.0
        ),
        repetition_penalty: float = Input(
            description="Penalty for repeated words in generated text; 1 is no penalty, values greater than 1 discourage repetition, less than 1 encourage it.",
            ge=0.01,
            le=5,
            default=1
        )
        ) -> List[str]:
        input = self.tokenizer(prompt, return_tensors="pt").input_ids.to(self.device)

        outputs = self.model.generate(
            input,
            num_return_sequences=n,
            max_length=max_length,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=repetition_penalty
        )
        out = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
        return out
```

The [AutoTokenizer](https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained) used above should work for all Transformers models.

If you want to use a Transformers model other than Flan-T5, you'll need to specify the `model` class to use. For example, if you're [using a GPT-J model](https://huggingface.co/EleutherAI/gpt-j-6b#how-to-use), you'll want to use `AutoModelForCausalLM` instead of `T5ForConditionalGeneration`. See the [Transformers docs](https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto#transformers.AutoModel.from_pretrained) for more details.

## [Anchor for step-6-download-weights](https://replicate.com/docs/guides/push-a-transformers-model\#step-6-download-weights) Step 6: Download weights

Next you'll create a script that uses the `transformers` library to download pretrained weights.

Create a file for the script:

Copy

```
mkdir script
touch script/download_weights
chmod +x script/download_weights # makes the file executable
```

Paste the following code into the `script/download_weights` file:

Copy

```
#!/usr/bin/env python

import os
import shutil
from transformers import T5ForConditionalGeneration, T5Tokenizer

CACHE_DIR = 'weights'

if os.path.exists(CACHE_DIR):
    shutil.rmtree(CACHE_DIR)

os.makedirs(CACHE_DIR)

model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xl", cache_dir=CACHE_DIR)
tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xl", cache_dir=CACHE_DIR)`
```

Run the script to download the weights:

Copy

```
cog run script/download_weights
```

This process will take a while to run but you'll only need to run it once, as it will cache the downloaded dependencies on disk. Get up and stretch, grab yourself a snack, or use this opportunity to add metadata to the model page you created on Replicate in Step 1 by adding a title, README, GitHub repository URL, etc.

## [Anchor for step-7-run-your-model](https://replicate.com/docs/guides/push-a-transformers-model\#step-7-run-your-model) Step 7: Run your model

Now that you've downloaded the weights, you can run the model locally with Cog:

Copy

```
cog predict -i prompt="Q: Answer the following yes/no question by reasoning step-by-step. Can a dog drive a car?"
```

This will run the model locally and return output text.

## [Anchor for step-8-push-your-model](https://replicate.com/docs/guides/push-a-transformers-model\#step-8-push-your-model) Step 8: Push your model

Now that you've created your model, it's time to push it to Replicate.

First you'll need to authenticate:

Copy

```
cog login
```

Then push your model using the name you specified in Step 1:

Copy

```
cog push r8.im/<your-username>/<your-model-name>
```

## [Anchor for step-9-use-your-model](https://replicate.com/docs/guides/push-a-transformers-model\#step-9-use-your-model) Step 9: Use your model

Your model is now live! ðŸš€

You can run the model from the website by clicking the "Demo" tab on the model page, or you can use the HTTP API to run the model from your own code.

Click the "API" tab on your model page to see example code for running the model:

![](https://user-images.githubusercontent.com/2289/231015658-cac5cceb-3d06-4090-884e-ca6db00321e4.png)

## [Anchor for next-steps](https://replicate.com/docs/guides/push-a-transformers-model\#next-steps) Next steps

Now that you have your own model, see what else you can do with it!

To see what models you can use, [check out the Transformers docs on Hugging Face](https://huggingface.co/docs/transformers).

If you need inspiration or guidance, jump into our [Discord](https://discord.gg/replicate).

[Next:Build a webhook notifier with Val Town](https://replicate.com/docs/guides/build-a-webhook-notifier-with-val-town)